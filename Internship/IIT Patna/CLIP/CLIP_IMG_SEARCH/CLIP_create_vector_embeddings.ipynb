{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-25 19:48:05.927899: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-25 19:48:06.070079: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-25 19:48:06.071701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-25 19:48:06.090928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-25 19:48:06.152435: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-25 19:48:07.150725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from datasets import Dataset, Image\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Union\n",
        "from transformers import CLIPProcessor, CLIPModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CLIPConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"openai/clip-vit-base-patch32\",\n",
            "  \"architectures\": [\n",
            "    \"CLIPModel\"\n",
            "  ],\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 2.6592,\n",
            "  \"model_type\": \"clip\",\n",
            "  \"projection_dim\": 512,\n",
            "  \"text_config\": {\n",
            "    \"bos_token_id\": 0,\n",
            "    \"dropout\": 0.0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"model_type\": \"clip_text_model\"\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.0\",\n",
            "  \"vision_config\": {\n",
            "    \"dropout\": 0.0,\n",
            "    \"model_type\": \"clip_vision_model\"\n",
            "  }\n",
            "}\n",
            "\n",
            "logit_scale torch.Size([])\n",
            "text_model.embeddings.token_embedding.weight torch.Size([49408, 512])\n",
            "text_model.embeddings.position_embedding.weight torch.Size([77, 512])\n",
            "text_model.encoder.layers.0.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.0.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.0.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.0.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.0.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.0.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.0.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.0.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.0.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.0.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.0.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.0.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.0.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.0.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.0.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.0.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.1.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.1.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.1.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.1.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.1.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.1.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.1.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.1.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.1.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.1.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.1.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.1.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.1.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.1.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.1.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.1.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.2.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.2.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.2.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.2.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.2.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.2.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.2.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.2.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.2.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.2.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.2.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.2.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.2.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.2.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.2.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.2.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.3.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.3.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.3.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.3.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.3.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.3.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.3.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.3.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.3.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.3.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.3.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.3.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.3.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.3.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.3.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.3.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.4.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.4.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.4.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.4.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.4.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.4.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.4.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.4.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.4.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.4.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.4.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.4.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.4.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.4.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.4.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.4.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.5.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.5.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.5.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.5.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.5.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.5.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.5.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.5.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.5.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.5.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.5.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.5.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.5.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.5.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.5.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.5.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.6.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.6.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.6.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.6.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.6.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.6.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.6.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.6.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.6.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.6.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.6.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.6.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.6.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.6.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.6.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.6.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.7.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.7.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.7.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.7.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.7.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.7.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.7.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.7.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.7.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.7.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.7.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.7.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.7.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.7.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.7.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.7.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.8.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.8.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.8.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.8.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.8.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.8.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.8.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.8.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.8.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.8.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.8.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.8.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.8.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.8.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.8.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.8.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.9.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.9.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.9.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.9.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.9.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.9.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.9.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.9.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.9.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.9.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.9.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.9.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.9.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.9.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.9.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.9.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.10.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.10.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.10.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.10.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.10.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.10.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.10.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.10.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.10.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.10.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.10.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.10.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.10.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.10.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.10.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.10.layer_norm2.bias torch.Size([512])\n",
            "text_model.encoder.layers.11.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.11.self_attn.k_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.11.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.11.self_attn.v_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.11.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.11.self_attn.q_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.11.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "text_model.encoder.layers.11.self_attn.out_proj.bias torch.Size([512])\n",
            "text_model.encoder.layers.11.layer_norm1.weight torch.Size([512])\n",
            "text_model.encoder.layers.11.layer_norm1.bias torch.Size([512])\n",
            "text_model.encoder.layers.11.mlp.fc1.weight torch.Size([2048, 512])\n",
            "text_model.encoder.layers.11.mlp.fc1.bias torch.Size([2048])\n",
            "text_model.encoder.layers.11.mlp.fc2.weight torch.Size([512, 2048])\n",
            "text_model.encoder.layers.11.mlp.fc2.bias torch.Size([512])\n",
            "text_model.encoder.layers.11.layer_norm2.weight torch.Size([512])\n",
            "text_model.encoder.layers.11.layer_norm2.bias torch.Size([512])\n",
            "text_model.final_layer_norm.weight torch.Size([512])\n",
            "text_model.final_layer_norm.bias torch.Size([512])\n",
            "vision_model.embeddings.class_embedding torch.Size([768])\n",
            "vision_model.embeddings.patch_embedding.weight torch.Size([768, 3, 32, 32])\n",
            "vision_model.embeddings.position_embedding.weight torch.Size([50, 768])\n",
            "vision_model.pre_layrnorm.weight torch.Size([768])\n",
            "vision_model.pre_layrnorm.bias torch.Size([768])\n",
            "vision_model.encoder.layers.0.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.0.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.0.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.0.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.0.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.0.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.0.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.0.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.0.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.0.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.0.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.0.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.0.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.0.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.0.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.1.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.1.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.1.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.1.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.1.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.1.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.1.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.1.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.1.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.1.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.1.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.1.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.1.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.1.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.1.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.1.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.2.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.2.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.2.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.2.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.2.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.2.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.2.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.2.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.2.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.2.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.2.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.2.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.2.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.2.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.2.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.2.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.3.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.3.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.3.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.3.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.3.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.3.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.3.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.3.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.3.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.3.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.3.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.3.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.3.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.3.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.3.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.3.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.4.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.4.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.4.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.4.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.4.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.4.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.4.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.4.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.4.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.4.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.4.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.4.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.4.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.4.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.4.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.4.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.5.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.5.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.5.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.5.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.5.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.5.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.5.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.5.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.5.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.5.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.5.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.5.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.5.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.5.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.5.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.5.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.6.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.6.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.6.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.6.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.6.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.6.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.6.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.6.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.6.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.6.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.6.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.6.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.6.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.6.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.6.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.6.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.7.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.7.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.7.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.7.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.7.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.7.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.7.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.7.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.7.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.7.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.7.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.7.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.7.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.7.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.7.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.7.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.8.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.8.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.8.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.8.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.8.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.8.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.8.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.8.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.8.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.8.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.8.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.8.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.8.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.8.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.8.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.8.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.9.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.9.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.9.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.9.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.9.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.9.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.9.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.9.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.9.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.9.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.9.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.9.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.9.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.9.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.9.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.9.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.10.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.10.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.10.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.10.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.10.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.10.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.10.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.10.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.10.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.10.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.10.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.10.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.10.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.10.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.10.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.10.layer_norm2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.11.self_attn.k_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.11.self_attn.k_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.11.self_attn.v_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.11.self_attn.v_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.11.self_attn.q_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.11.self_attn.q_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.11.self_attn.out_proj.weight torch.Size([768, 768])\n",
            "vision_model.encoder.layers.11.self_attn.out_proj.bias torch.Size([768])\n",
            "vision_model.encoder.layers.11.layer_norm1.weight torch.Size([768])\n",
            "vision_model.encoder.layers.11.layer_norm1.bias torch.Size([768])\n",
            "vision_model.encoder.layers.11.mlp.fc1.weight torch.Size([3072, 768])\n",
            "vision_model.encoder.layers.11.mlp.fc1.bias torch.Size([3072])\n",
            "vision_model.encoder.layers.11.mlp.fc2.weight torch.Size([768, 3072])\n",
            "vision_model.encoder.layers.11.mlp.fc2.bias torch.Size([768])\n",
            "vision_model.encoder.layers.11.layer_norm2.weight torch.Size([768])\n",
            "vision_model.encoder.layers.11.layer_norm2.bias torch.Size([768])\n",
            "vision_model.post_layernorm.weight torch.Size([768])\n",
            "vision_model.post_layernorm.bias torch.Size([768])\n",
            "visual_projection.weight torch.Size([512, 768])\n",
            "text_projection.weight torch.Size([512, 512])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "print(model.config)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "image_path = os.listdir(\"/home/abhinav/Documents/Work/2 Hobby_projects/Datasets/Flicker_8k/Images/\")\n",
        "image_path = ['/home/abhinav/Documents/Work/2 Hobby_projects/Datasets/Flicker_8k/Images/' + path for path in image_path if '.jpg' in path]\n",
        "image_path.sort()\n",
        "\n",
        "# captions_df = pd.read_csv('captions.csv')\n",
        "captions_df = pd.read_csv('/home/abhinav/Documents/Work/2 Hobby_projects/Datasets/Flicker_8k/captions.csv')\n",
        "\n",
        "\n",
        "\n",
        "def encode_images(images: Union[List[str], List[PIL.Image.Image]], batch_size: int):\n",
        "    def transform_fn(el):\n",
        "        if isinstance(el['image'], PIL.Image.Image):\n",
        "            imgs = el['image']\n",
        "        else:\n",
        "            imgs = [Image().decode_example(_) for _ in el['image']]\n",
        "        return preprocess(images=imgs, return_tensors='pt')\n",
        "        \n",
        "    dataset = Dataset.from_dict({'image': images})\n",
        "    dataset = dataset.cast_column('image',Image(decode=False)) if isinstance(images[0], str) else dataset       \n",
        "    dataset.set_format('torch')\n",
        "    dataset.set_transform(transform_fn)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    image_embeddings = []\n",
        "    pbar = tqdm(total=len(images) // batch_size, position=0)\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k:v.to(device) for k,v in batch.items()}\n",
        "            image_embeddings.extend(model.get_image_features(**batch).detach().cpu().numpy())\n",
        "            pbar.update(1)\n",
        "        pbar.close()\n",
        "    return np.stack(image_embeddings)\n",
        "\n",
        "\n",
        "def encode_text( text: List[str], batch_size: int):\n",
        "    dataset = Dataset.from_dict({'text': text})\n",
        "    dataset = dataset.map(lambda el: preprocess(text=el['text'], return_tensors=\"pt\",\n",
        "                                                        max_length=77, padding=\"max_length\", truncation=True),\n",
        "                            batched=True,\n",
        "                            remove_columns=['text'])\n",
        "    dataset.set_format('torch')\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    text_embeddings = []\n",
        "    pbar = tqdm(total=len(text) // batch_size, position=0)\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            text_embeddings.extend(model.get_text_features(**batch).detach().cpu().numpy())\n",
        "            pbar.update(1)\n",
        "        pbar.close()\n",
        "    return np.stack(text_embeddings)\n",
        "\n",
        "vector_embedding = np.array(encode_images(image_path,32))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open('/home/abhinav/Desktop/AIProjects/Internship/IIT Patna/CLIP/flicker8k_image_embeddings.pkl','wb') as f:\n",
        "    pickle.dump(vector_embedding, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
