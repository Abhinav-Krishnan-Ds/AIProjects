{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 11:18:43.180674: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 11:18:43.302727: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-12 11:18:43.302818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-12 11:18:43.322444: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 11:18:43.366391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-12 11:18:44.160809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gpt_download3 import load_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import ssl\n",
    "# import zipfile\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "# zip_path = \"sms_spam_collection.zip\"\n",
    "# extracted_path = \"sms_spam_collection\"\n",
    "# data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "#     if data_file_path.exists():\n",
    "#         print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "#         return\n",
    "\n",
    "#     # Create an unverified SSL context\n",
    "#     ssl_context = ssl._create_unverified_context()\n",
    "\n",
    "#     # Downloading the file\n",
    "#     with urllib.request.urlopen(url, context=ssl_context) as response:\n",
    "#         with open(zip_path, \"wb\") as out_file:\n",
    "#             out_file.write(response.read())\n",
    "\n",
    "#     # Unzipping the file\n",
    "#     with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "#         zip_ref.extractall(extracted_path)\n",
    "\n",
    "#     # Add .tsv file extension\n",
    "#     original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "#     os.rename(original_file_path, data_file_path)\n",
    "#     print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "# download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sms_spam_collection/SMSSpamCollection.tsv\", sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    return pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     747\n",
       "spam    747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to tokenization\n",
    "\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>1</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>1</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>1</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?\n",
       "...     ...                                                ...\n",
       "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, valid_frac):\n",
    "    # shuffle the dataframe\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    train_end = int(len(df)*train_frac)\n",
    "    valid_end = train_end + int(len(df)*valid_frac)\n",
    "    \n",
    "    train_df = df[:train_end]\n",
    "    valid_df = df[train_end:valid_end]\n",
    "    test_df = df[valid_end:]\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n",
      "149\n",
      "300\n",
      "Summ of all split:  1494\n"
     ]
    }
   ],
   "source": [
    "a = len(train_df)\n",
    "b = len(validation_df)\n",
    "c = len(test_df)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(\"Summ of all split: \", a+b+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_text()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id]*(self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_text(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            if len(encoded_text) > max_length:\n",
    "                max_length = len(encoded_text)\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=None,\n",
    "    pad_token_id=50256\n",
    ")\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)\n",
    "print(val_dataset.max_length)\n",
    "print(test_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader one batch shape:\n",
      "torch.Size([8, 120])\n",
      "torch.Size([8])\n",
      "Like this there are: 130 batches in total inside train loader\n",
      "Like this there are: 18 batches in total inside valid loader\n",
      "Like this there are: 37 batches in total inside test loader\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader one batch shape:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(input_batch.shape)\n",
    "print(target_batch.shape)\n",
    "\n",
    "print(f\"Like this there are: {len(train_loader)} batches in total inside train loader\")\n",
    "print(f\"Like this there are: {len(valid_loader)} batches in total inside valid loader\")\n",
    "print(f\"Like this there are: {len(test_loader)} batches in total inside test loader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings, params = load_gpt(models_dir=\"/home/abhinav/Documents/Work/2 Hobby_projects/Models/gpt2\", model_size=\"124M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(settings)\n",
    "print(params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 128,\n",
    "    \"emb_dim\" : 708,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 11,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': True}\n"
     ]
    }
   ],
   "source": [
    "print(NEW_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = 1e-05\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-mean) / (torch.sqrt(variance + self.eps))\n",
    "\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return  0.5 * x * ( 1 + torch.tanh( torch.sqrt(  torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)  ) ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.ff = FeedForward(cfg=cfg)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[ TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"]) ]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.token_emb(in_idx).to(device=device)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Left shape: {left.shape}, Right shape: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text, allowed_special={\"<|endoftoken|>\"})\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0)\n",
    "    return tokens\n",
    "\n",
    "def token_ids_to_text(idx, tokenizer):\n",
    "    text = idx.squeeze(0)\n",
    "    return tokenizer.decode(text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Every effort moves you\"\n",
    "text2 = \"Is the following text spam? 'You are a winner, you have been specially selected to recieve 1000 dollars' ? Answer it with a yes or no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn. By now there's a very small delay before you gain\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(text1, tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Is the following text spam? 'You are a winner, you have been specially selected to recieve 1000 dollars' ? Answer it with a yes or no as you were invited to use this program (and that means you want to receive more). In your own time (no matter\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(text2, tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in gpt.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "gpt.out_head = nn.Linear(in_features=NEW_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in gpt.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in gpt.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     outputs = gpt(inputs)\n",
    "\n",
    "# print(\"Outputs:\\n\", outputs)\n",
    "# print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)#\n",
    "# print(\"Last output: \", outputs[:, -1 ,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(dataloader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device=device), target_batch.to(device=device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1 ,:]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 53.75%\n",
      "Test accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(valid_loader, gpt, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, gpt, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device=device), target_batch.to(device=device)\n",
    "    logits = model(input_batch)[:, -1 ,:]\n",
    "    # print(\"Input to model:\")\n",
    "    # for k, i in enumerate(input_batch):\n",
    "    #     print(\"Looking at \", k, \" example\\n\")\n",
    "    #     print(\"Its target value is: \", target_batch[k])\n",
    "    #     print(token_ids_to_text(i, tokenizer=tokenizer))\n",
    "    # print(\"logits:\")\n",
    "    # print(logits.shape)\n",
    "    # print(logits)\n",
    "    # print(\"target_batch\")\n",
    "    # print(target_batch.shape)\n",
    "    # print(target_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(dataloader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch=input_batch, target_batch=target_batch, model=model, device=device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.605\n",
      "Validation loss: 2.474\n",
      "Test loss: 2.548\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "    val_loss = calc_loss_loader(valid_loader, gpt, device)\n",
    "    test_loss = calc_loss_loader(test_loader, gpt, device)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([[0., 1.], [0., 1.]])\n",
    "# a = torch.argmax(a, dim=-1)\n",
    "# print(a.shape)\n",
    "# b = torch.tensor([[0.], [0.]])\n",
    "\n",
    "# print(torch.nn.functional.cross_entropy(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, valid_loader, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, eval_iter)\n",
    "        val_loss = calc_loss_loader(valid_loader, model, device, eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"(Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "(Step 000000): Train loss 0.780, Val loss 0.915\n",
      "(Step 000050): Train loss 0.553, Val loss 0.564\n",
      "(Step 000100): Train loss 0.097, Val loss 0.378\n",
      "Training accuracy: 87.50% | Validation accuracy: 100.00%\n",
      "Epoch:  1\n",
      "(Step 000150): Train loss 0.026, Val loss 0.032\n",
      "(Step 000200): Train loss 0.021, Val loss 0.131\n",
      "(Step 000250): Train loss 0.022, Val loss 0.033\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Epoch:  2\n",
      "(Step 000300): Train loss 0.020, Val loss 0.040\n",
      "(Step 000350): Train loss 0.101, Val loss 0.046\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Epoch:  3\n",
      "(Step 000400): Train loss 0.202, Val loss 0.000\n",
      "(Step 000450): Train loss 0.070, Val loss 0.188\n",
      "(Step 000500): Train loss 0.064, Val loss 0.100\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Epoch:  4\n",
      "(Step 000550): Train loss 0.107, Val loss 0.012\n",
      "(Step 000600): Train loss 0.003, Val loss 0.003\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed\n",
      "Total training time: 1.39\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    gpt, train_loader,\n",
    "    valid_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    eval_freq=50, \n",
    "    eval_iter=5\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = (end_time - start_time) / 60\n",
    "\n",
    "print(\"Training completed\")\n",
    "print(f\"Total training time: {execution_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Create a second x-axis for examples seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.85%\n",
      "Validation accuracy: 99.31%\n",
      "Test accuracy: 97.30%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device)\n",
    "val_accuracy = calc_accuracy_loader(valid_loader, gpt, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, gpt, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, device, tokenizer, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    if max_length > supported_context_length:\n",
    "        max_length = supported_context_length\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1 ,:]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"Spam\" if predicted_label == 1 else \"Not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam\n",
      "Not spam\n",
      "Not spam\n",
      "Not spam\n",
      "Not spam\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    "    ),\n",
    "    (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"),\n",
    "    (\"Hey winner champ, we are gonna miss you\"),\n",
    "    (\"Won 1000 dollars, please give otp\"),\n",
    "    (\"You are selected as the winner\")\n",
    "]\n",
    "\n",
    "for i in texts:\n",
    "    print(classify_review(\n",
    "        text=i,\n",
    "        model=gpt,\n",
    "        device=device,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=train_dataset.max_length,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
