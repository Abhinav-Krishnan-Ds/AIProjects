{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:11:48.614642: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-06 14:11:48.756830: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-06 14:11:48.756938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-06 14:11:48.776600: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 14:11:48.817670: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-06 14:11:49.725761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from gpt_download3 import load_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All downloaded llm through ollama\n",
    "\n",
    "# 1) llama3.2:1b\n",
    "# 2) llama3:latest\n",
    "# 3) gemma:2b\n",
    "# 4) mistral:latest\n",
    "# 5) mapler/gpt2:latest\n",
    "\n",
    "# total size of all the models combined: 12.1 gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-data-with-response.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To score the text to sign gloss\n",
    "# scores_total = 0\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     a = test_data[i][\"output\"]\n",
    "#     b = test_data[i][\"model_response\"]\n",
    "#     la = len(a)\n",
    "#     lb = len(b)\n",
    "#     same_char = 0\n",
    "#     total_char = la+lb\n",
    "#     for j in range(min(la, lb)):\n",
    "#         if a[j] == b[j]:\n",
    "#             same_char += 2\n",
    "#     scores_total += (int((same_char/total_char)*100))\n",
    "\n",
    "# print(\"Average score: \", int(scores_total/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.19053627645285995\n"
     ]
    }
   ],
   "source": [
    "# Finding BLEU score\n",
    "\n",
    "\n",
    "reference = [['I', 'EAT', 'AN', 'APPLE']]\n",
    "candidate = ['I', 'EAT', 'APPLE']\n",
    "score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score:  0.02376306472648045\n"
     ]
    }
   ],
   "source": [
    "total_score = 0\n",
    "\n",
    "for i in range(10):\n",
    "    a = test_data[i][\"output\"].split(\" \")\n",
    "    b = test_data[i][\"model_response\"].split(\" \")\n",
    "    # print(\"Output: \", a)\n",
    "    # print(\"Model response: \", b)\n",
    "    score = sentence_bleu(a, b, smoothing_function=SmoothingFunction().method3)\n",
    "    # print(\"Score of similarity: \", score)\n",
    "    # print(\"--------------\")\n",
    "    total_score += score\n",
    "\n",
    "print(\"Average score: \", total_score/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_llm = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=larger_llm,\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = larger_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n",
      "Small checkpoint\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i in range(l):\n",
    "    if i % 100 == 0:\n",
    "        print(\"Small checkpoint\")\n",
    "    elif i % 1000 == 0:\n",
    "        print(\"Bigger checkpoint\")\n",
    "    result = query_model(\n",
    "            \"\"\"Score similarity of 2nd text based on 1st text out of 100( First one is reference, second one is candidate) and Similarity score should be given at the starting as SCORE: <> AND NO EXPLANATION NEEDED 1:\"\"\" + test_data[i][\"output\"] + \" 2: \" + test_data[i][\"model_response\"],\n",
    "            model\n",
    "            )\n",
    "    try:\n",
    "        a = result.split(\": \")\n",
    "        scores.append(int(a[1]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3034\n"
     ]
    }
   ],
   "source": [
    "print(len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 50, 80, 50, 50, 80, 42, 10, 50, 80, 42, 40, 20, 42, 50, 80, 42, 80, 74, 80, 60, 50, 42, 42, 42, 80, 50, 80, 50, 80, 14, 60, 80, 50, 50, 50, 50, 20, 60, 50, 80, 80, 80, 90, 14, 67, 50, 42, 50, 50, 50, 74, 50, 90, 80, 50, 80, 80, 80, 50, 50, 60, 42, 20, 50, 50, 42, 60, 50, 80, 80, 20, 80, 80, 20, 60, 50, 50, 42, 20, 80, 50, 50, 50, 50, 50, 50, 50, 34, 80, 10, 50, 42, 50, 80, 90, 50, 50, 50, 50, 90, 20, 50, 42, 60, 74, 80, 80, 42, 50, 30, 40, 50, 80, 50, 10, 50, 42, 80, 50, 42, 60, 80, 50, 42, 80, 80, 20, 20, 50, 50, 74, 42, 50, 80, 74, 50, 50, 50, 42, 50, 42, 50, 50, 50, 80, 20, 50, 74, 42, 100, 42, 80, 67, 50, 50, 60, 50, 74, 50, 50, 50, 50, 42, 74, 20, 50, 50, 50, 0, 42, 80, 50, 50, 80, 50, 50, 50, 42, 42, 42, 50, 67, 50, 50, 50, 50, 50, 42, 50, 50, 80, 50, 74, 50, 60, 50, 42, 34, 50, 20, 20, 90, 90, 42, 74, 50, 80, 67, 50, 50, 74, 50, 50, 50, 50, 74, 80, 80, 42, 20, 50, 50, 50, 20, 50, 50, 80, 74, 20, 50, 80, 50, 96, 80, 50, 50, 74, 20, 42, 80, 100, 50, 50, 80, 74, 14, 50, 50, 74, 50, 42, 74, 50, 50, 80, 50, 50, 80, 80, 50, 50, 50, 50, 80, 60, 80, 90, 10, 50, 20, 50, 50, 42, 50, 50, 20, 50, 50, 50, 80, 20, 80, 42, 50, 80, 50, 42, 80, 20, 20, 50, 80, 50, 80, 50, 50, 20, 67, 96, 42, 50, 42, 74, 50, 42, 50, 50, 50, 50, 50, 90, 50, 50, 50, 50, 14, 50, 60, 74, 80, 50, 50, 40, 80, 74, 80, 85, 50, 20, 50, 60, 50, 42, 50, 74, 42, 80, 42, 42, 74, 42, 0, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 10, 66, 50, 42, 50, 14, 67, 90, 20, 50, 80, 80, 80, 42, 74, 74, 50, 50, 50, 50, 74, 0, 50, 50, 50, 20, 50, 80, 74, 50, 80, 74, 80, 50, 50, 74, 50, 80, 42, 20, 80, 80, 50, 50, 20, 50, 50, 90, 50, 80, 42, 85, 50, 50, 50, 80, 50, 50, 50, 80, 50, 50, 50, 80, 42, 42, 50, 70, 50, 42, 67, 20, 50, 50, 50, 50, 74, 14, 80, 74, 50, 80, 10, 42, 50, 74, 60, 74, 50, 74, 74, 20, 60, 80, 40, 50, 50, 50, 50, 80, 42, 42, 42, 42, 85, 80, 50, 50, 80, 80, 60, 50, 42, 50, 50, 50, 80, 50, 14, 20, 42, 42, 50, 42, 80, 42, 20, 50, 50, 60, 80, 50, 67, 50, 50, 74, 50, 42, 50, 50, 50, 80, 50, 42, 50, 80, 50, 42, 20, 80, 50, 50, 50, 80, 50, 40, 50, 50, 74, 74, 50, 74, 42, 42, 50, 50, 60, 67, 74, 10, 0, 50, 50, 50, 74, 50, 50, 50, 50, 50, 50, 50, 50, 50, 20, 42, 74, 80, 50, 50, 50, 50, 50, 50, 50, 80, 80, 50, 50, 80, 60, 50, 80, 50, 60, 20, 50, 50, 50, 50, 50, 50, 50, 74, 0, 80, 90, 50, 90, 42, 50, 50, 50, 80, 50, 80, 50, 40, 50, 80, 60, 60, 40, 42, 50, 20, 50, 20, 50, 50, 80, 42, 40, 20, 40, 40, 50, 50, 50, 80, 20, 20, 40, 40, 40, 74, 40, 20, 50, 50, 20, 80, 10, 50, 50, 50, 42, 80, 20, 50, 60, 80, 60, 74, 60, 80, 50, 50, 80, 50, 90, 10, 42, 42, 50, 20, 80, 20, 72, 50, 50, 50, 74, 80, 80, 50, 60, 80, 50, 80, 50, 90, 50, 80, 50, 50, 20, 50, 50, 80, 50, 50, 50, 20, 60, 80, 80, 42, 50, 50, 80, 80, 50, 80, 80, 80, 80, 50, 50, 80, 10, 50, 80, 80, 50, 74, 42, 42, 80, 60, 42, 50, 64, 80, 50, 80, 80, 74, 74, 40, 80, 40, 0, 14, 80, 74, 50, 40, 42, 80, 60, 0, 50, 74, 50, 80, 80, 10, 50, 50, 60, 50, 42, 50, 67, 20, 42, 60, 80, 90, 50, 42, 67, 74, 14, 50, 50, 60, 80, 20, 90, 74, 67, 50, 20, 20, 42, 42, 50, 42, 80, 0, 80, 50, 50, 50, 42, 20, 50, 80, 20, 50, 74, 10, 50, 10, 50, 40, 80, 50, 42, 50, 80, 14, 50, 40, 80, 50, 50, 80, 80, 50, 50, 85, 50, 80, 42, 85, 50, 50, 50, 74, 50, 50, 20, 42, 50, 60, 42, 80, 50, 50, 50, 50, 50, 50, 50, 80, 50, 50, 74, 60, 20, 50, 60, 50, 50, 50, 50, 74, 74, 90, 42, 50, 0, 50, 50, 67, 74, 10, 50, 60, 50, 80, 50, 50, 50, 50, 50, 80, 50, 50, 0, 50, 50, 42, 74, 80, 80, 80, 74, 80, 80, 50, 80, 50, 50, 42, 42, 50, 50, 50, 42, 50, 10, 50, 74, 80, 42, 50, 50, 67, 50, 74, 80, 14, 50, 50, 50, 50, 67, 34, 50, 42, 20, 42, 50, 74, 80, 42, 42, 40, 50, 50, 50, 80, 40, 42, 50, 74, 50, 20, 10, 80, 42, 74, 50, 20, 50, 50, 40, 20, 42, 80, 50, 85, 80, 42, 10, 50, 80, 74, 50, 20, 20, 80, 74, 50, 20, 50, 20, 60, 90, 20, 20, 50, 50, 50, 50, 50, 74, 50, 96, 74, 80, 80, 50, 80, 42, 50, 96, 74, 0, 80, 95, 50, 80, 20, 50, 42, 0, 80, 42, 42, 50, 50, 80, 95, 90, 50, 20, 50, 80, 50, 80, 90, 50, 80, 40, 20, 74, 42, 50, 50, 74, 74, 50, 0, 50, 74, 42, 42, 74, 42, 50, 80, 95, 20, 80, 90, 50, 80, 42, 80, 42, 80, 74, 50, 80, 90, 42, 90, 50, 50, 90, 50, 80, 74, 74, 80, 80, 20, 50, 50, 74, 0, 74, 85, 74, 50, 40, 80, 80, 80, 50, 42, 50, 50, 50, 50, 80, 80, 10, 42, 80, 0, 50, 50, 42, 80, 50, 50, 42, 80, 50, 0, 42, 50, 74, 50, 50, 67, 50, 74, 42, 50, 67, 67, 14, 42, 80, 42, 10, 42, 80, 50, 50, 50, 50, 80, 10, 42, 80, 67, 42, 50, 80, 80, 20, 50, 10, 50, 50, 50, 50, 74, 80, 60, 80, 80, 74, 80, 50, 67, 42, 50, 67, 50, 0, 50, 50, 50, 42, 50, 50, 80, 80, 50, 74, 50, 50, 90, 50, 74, 80, 80, 50, 90, 80, 67, 50, 50, 80, 80, 20, 60, 96, 80, 50, 80, 80, 80, 42, 50, 90, 14, 50, 50, 20, 50, 50, 80, 20, 74, 50, 0, 50, 80, 60, 80, 50, 50, 20, 50, 82, 50, 80, 50, 50, 80, 50, 50, 60, 50, 50, 50, 72, 50, 20, 10, 50, 50, 50, 50, 50, 50, 42, 50, 50, 42, 25, 0, 74, 80, 50, 80, 74, 80, 42, 20, 42, 50, 50, 50, 50, 80, 14, 50, 0, 0, 80, 42, 20, 50, 50, 20, 50, 50, 80, 40, 42, 74, 42, 50, 80, 50, 50, 60, 42, 50, 20, 20, 10, 50, 42, 90, 74, 50, 50, 50, 20, 50, 80, 42, 50, 80, 42, 50, 10, 50, 50, 50, 50, 50, 80, 10, 50, 90, 67, 50, 50, 50, 50, 0, 50, 50, 42, 40, 80, 80, 50, 80, 50, 50, 72, 42, 50, 74, 20, 42, 80, 42, 43, 60, 80, 40, 50, 50, 50, 50, 40, 50, 74, 42, 85, 80, 80, 20, 74, 50, 20, 74, 74, 74, 80, 20, 74, 80, 80, 80, 74, 42, 80, 74, 80, 74, 74, 42, 50, 80, 80, 50, 74, 50, 42, 50, 80, 40, 60, 0, 50, 20, 40, 74, 50, 50, 90, 67, 50, 0, 50, 50, 42, 50, 40, 42, 50, 80, 50, 50, 50, 50, 50, 50, 50, 75, 50, 50, 50, 50, 90, 50, 50, 50, 50, 74, 50, 50, 50, 50, 50, 20, 14, 50, 74, 50, 80, 50, 74, 20, 90, 50, 50, 42, 50, 80, 42, 42, 50, 74, 42, 50, 20, 42, 80, 50, 80, 100, 50, 50, 50, 90, 50, 50, 80, 40, 42, 80, 42, 90, 50, 80, 50, 42, 50, 80, 50, 20, 50, 74, 50, 50, 80, 50, 50, 50, 80, 50, 80, 42, 80, 90, 50, 40, 74, 50, 80, 50, 60, 50, 50, 90, 80, 50, 60, 50, 74, 50, 14, 50, 50, 42, 80, 50, 50, 50, 50, 80, 10, 42, 50, 40, 67, 90, 67, 20, 74, 80, 80, 50, 50, 74, 80, 40, 10, 80, 50, 50, 80, 80, 42, 80, 50, 90, 50, 74, 50, 80, 90, 90, 42, 80, 50, 67, 67, 50, 60, 80, 60, 100, 20, 50, 80, 50, 50, 20, 42, 50, 42, 42, 90, 50, 50, 74, 50, 50, 80, 67, 74, 74, 80, 50, 50, 50, 80, 50, 50, 74, 50, 50, 80, 80, 20, 50, 14, 50, 50, 0, 50, 42, 80, 90, 50, 50, 64, 50, 80, 42, 50, 80, 50, 80, 50, 50, 50, 50, 50, 80, 42, 42, 50, 67, 80, 50, 50, 50, 50, 10, 50, 50, 50, 50, 10, 74, 42, 50, 50, 50, 60, 42, 50, 50, 50, 50, 42, 50, 50, 50, 74, 74, 67, 67, 20, 50, 50, 50, 74, 42, 80, 80, 50, 80, 60, 20, 50, 50, 50, 50, 80, 14, 42, 50, 50, 50, 50, 20, 20, 50, 80, 80, 74, 40, 50, 50, 50, 50, 50, 20, 50, 50, 80, 50, 50, 50, 80, 0, 20, 74, 74, 50, 96, 50, 42, 42, 20, 20, 50, 50, 50, 50, 50, 80, 50, 80, 80, 50, 95, 50, 80, 42, 10, 50, 90, 20, 50, 42, 50, 50, 80, 42, 50, 42, 20, 74, 80, 42, 20, 50, 74, 80, 42, 42, 80, 50, 0, 80, 50, 50, 50, 50, 50, 50, 20, 80, 80, 50, 67, 85, 50, 50, 85, 42, 67, 80, 50, 42, 74, 50, 80, 74, 42, 80, 80, 0, 50, 50, 50, 80, 20, 50, 50, 80, 0, 42, 50, 80, 50, 80, 42, 25, 50, 42, 50, 80, 80, 80, 50, 50, 100, 50, 80, 80, 80, 42, 40, 20, 85, 50, 80, 80, 74, 50, 42, 74, 80, 80, 80, 95, 50, 74, 80, 90, 42, 80, 80, 80, 20, 67, 67, 80, 50, 50, 80, 80, 50, 80, 74, 80, 42, 80, 50, 74, 80, 20, 85, 80, 50, 90, 80, 10, 74, 50, 20, 80, 60, 80, 50, 50, 80, 0, 80, 50, 50, 74, 0, 74, 80, 74, 50, 74, 80, 74, 50, 80, 42, 80, 74, 50, 80, 80, 80, 162, 60, 74, 90, 80, 50, 42, 14, 50, 42, 80, 85, 50, 74, 80, 40, 80, 80, 50, 80, 20, 10, 90, 0, 50, 80, 50, 50, 50, 42, 50, 42, 50, 50, 80, 50, 14, 80, 74, 0, 50, 50, 42, 74, 50, 50, 50, 20, 42, 50, 80, 60, 95, 42, 50, 74, 50, 72, 90, 50, 80, 60, 42, 56, 42, 50, 74, 42, 74, 50, 50, 74, 80, 90, 50, 42, 10, 50, 50, 74, 50, 80, 50, 20, 50, 96, 50, 50, 50, 74, 80, 50, 90, 40, 42, 74, 50, 40, 50, 80, 74, 40, 74, 50, 42, 80, 74, 50, 80, 50, 50, 60, 50, 85, 80, 0, 80, 80, 96, 80, 72, 60, 20, 50, 80, 80, 60, 20, 42, 42, 42, 42, 50, 50, 74, 50, 50, 42, 20, 50, 42, 42, 50, 50, 50, 10, 50, 74, 50, 50, 50, 50, 20, 50, 50, 67, 74, 50, 42, 60, 90, 50, 42, 50, 20, 74, 0, 42, 60, 50, 20, 50, 80, 50, 50, 90, 50, 80, 50, 80, 50, 50, 50, 50, 50, 20, 50, 60, 50, 20, 74, 90, 80, 50, 60, 50, 42, 50, 80, 90, 50, 42, 20, 50, 80, 50, 80, 50, 50, 50, 80, 50, 50, 50, 50, 60, 0, 80, 42, 50, 80, 50, 50, 42, 67, 50, 14, 50, 0, 50, 42, 0, 80, 50, 20, 74, 10, 50, 50, 50, 80, 60, 50, 42, 50, 42, 80, 80, 42, 42, 60, 42, 60, 0, 42, 74, 80, 50, 42, 50, 20, 50, 50, 50, 80, 50, 50, 50, 74, 42, 80, 80, 80, 42, 42, 50, 74, 50, 50, 42, 50, 60, 20, 50, 90, 50, 50, 10, 50, 80, 50, 42, 50, 74, 42, 50, 50, 40, 20, 20, 74, 80, 80, 42, 42, 50, 85, 42, 50, 14, 50, 20, 80, 50, 74, 50, 50, 50, 42, 50, 42, 72, 50, 42, 42, 50, 80, 80, 74, 80, 50, 80, 50, 50, 80, 60, 42, 50, 50, 50, 40, 50, 42, 80, 42, 50, 50, 40, 90, 80, 50, 50, 80, 42, 50, 50, 60, 50, 50, 50, 50, 50, 42, 14, 50, 42, 80, 50, 50, 50, 20, 50, 50, 20, 80, 42, 50, 67, 74, 50, 42, 50, 10, 50, 42, 50, 50, 14, 50, 50, 50, 80, 50, 80, 20, 42, 50, 42, 20, 50, 50, 80, 50, 50, 50, 74, 50, 50, 50, 74, 50, 50, 50, 50, 50, 90, 80, 50, 50, 50, 80, 42, 42, 50, 14, 80, 80, 50, 67, 50, 50, 42, 74, 80, 74, 42, 85, 80, 74, 50, 50, 50, 50, 80, 80, 80, 74, 40, 50, 42, 42, 74, 50, 42, 74, 50, 50, 50, 14, 50, 42, 74, 50, 50, 75, 50, 50, 80, 50, 42, 74, 80, 20, 60, 74, 50, 50, 80, 50, 50, 80, 50, 50, 50, 50, 50, 80, 42, 23, 50, 40, 20, 80, 60, 80, 50, 80, 50, 67, 74, 50, 42, 50, 50, 42, 50, 50, 74, 50, 20, 50, 20, 60, 42, 50, 95, 50, 50, 20, 42, 50, 20, 20, 50, 50, 50, 50, 50, 50, 50, 14, 0, 50, 42, 50, 50, 50, 50, 42, 50, 50, 10, 50, 50, 50, 50, 50, 50, 85, 42, 80, 10, 80, 74, 80, 96, 80, 80, 50, 80, 42, 42, 60, 74, 80, 10, 42, 50, 20, 80, 40, 90, 10, 74, 74, 74, 50, 0, 80, 74, 74, 67, 50, 50, 50, 0, 50, 50, 80, 20, 74, 0, 74, 74, 42, 50, 42, 42, 67, 42, 50, 80, 74, 74, 50, 96, 50, 74, 50, 14, 90, 50, 42, 95, 50, 80, 50, 0, 50, 50, 50, 50, 80, 80, 42, 10, 90, 95, 50, 80, 20, 60, 42, 80, 74, 20, 50, 50, 50, 50, 80, 50, 50, 50, 67, 40, 42, 50, 50, 50, 42, 74, 50, 74, 50, 50, 56, 50, 50, 60, 50, 50, 80, 50, 50, 50, 42, 50, 50, 20, 74, 74, 20, 50, 60, 42, 42, 74, 50, 50, 50, 80, 50, 90, 50, 10, 80, 42, 42, 67, 74, 42, 80, 96, 74, 50, 50, 42, 50, 80, 42, 14, 80, 50, 80, 60, 42, 80, 50, 80, 50, 50, 80, 74, 80, 74, 42, 50, 50, 67, 50, 50, 20, 50, 50, 80, 0, 50, 50, 50, 40, 80, 42, 42, 20, 74, 50, 74, 50, 74, 80, 14, 50, 42, 50, 42, 50, 50, 50, 20, 80, 80, 90, 67, 50, 50, 50, 42, 95, 80, 42, 74, 14, 90, 80, 14, 20, 80, 50, 42, 42, 50, 50, 50, 80, 60, 0, 50, 74, 42, 80, 50, 96, 50, 20, 46, 80, 74, 10, 0, 67, 50, 80, 80, 40, 10, 80, 80, 42, 80, 50, 42, 80, 80, 80, 80, 42, 50, 50, 50, 95, 50, 42, 50, 20, 50, 80, 50, 50, 50, 72, 60, 10, 74, 60, 80, 42, 20, 20, 0, 74, 80, 42, 50, 80, 50, 50, 42, 80, 74, 90, 20, 80, 74, 80, 42, 14, 20, 10, 50, 74, 50, 80, 80, 50, 50, 50, 42, 42, 50, 50, 42, 75, 50, 50, 50, 74, 50, 80, 50, 20, 50, 50, 74, 50, 50, 50, 74, 80, 50, 42, 50, 80, 20, 50, 74, 85, 42, 50, 42, 74, 50, 10, 42, 50, 50, 74, 42, 42, 50, 80, 80, 50, 60, 50, 50, 50, 74, 42, 74, 42, 42, 72, 60, 80, 50, 50, 42, 50, 74, 20, 74, 50, 67, 20, 80, 80, 40, 50, 50, 42, 42, 60, 20, 80, 20, 14, 20, 42, 50, 50, 60, 50, 14, 74, 50, 0, 40, 50, 50, 20, 80, 50, 50, 80, 74, 80, 80, 50, 50, 50, 90, 42, 50, 42, 50, 50, 50, 50, 50, 50, 50, 20, 50, 50, 42, 50, 50, 42, 20, 50, 50, 50, 50, 50, 50, 70, 80, 67, 50, 20, 80, 50, 80, 42, 80, 80, 20, 80, 74, 50, 72, 80, 50, 90, 42, 50, 50, 90, 74, 80, 80, 60, 50, 50, 50, 50, 42, 50, 0, 50, 90, 50, 74, 50, 14, 80, 20, 80, 50, 80, 80, 74, 80, 70, 50, 50, 50, 20, 80, 80, 80, 60, 80, 50, 42, 40, 80, 80, 10, 80, 20, 74, 80, 42, 56, 50, 20, 0, 50, 20, 50, 50, 42, 60, 42, 74, 50, 42, 20, 42, 20, 50, 42, 0, 20, 50, 74, 67, 50, 60, 80, 74, 14, 42, 90, 60, 74, 42, 42, 42, 0, 74, 90, 42, 50, 50, 20, 50, 60, 34, 50, 0, 50, 50, 80, 50, 80, 20, 74, 50, 50, 20, 80, 74, 67, 42, 80, 74, 20, 50, 80, 80, 50, 90, 50, 42, 50, 42, 67, 0, 50, 50, 50, 50, 50, 10, 50, 50, 80, 50, 80, 50, 80, 80, 20, 50, 80, 50, 42, 80, 74, 42, 50, 85, 14, 50, 42, 50, 50, 100, 80, 42, 90, 42, 74, 80, 50, 10, 50, 50, 42, 74, 50, 80, 67, 50, 20, 50, 50, 50, 0, 50, 50, 74, 40, 50, 50, 42, 50, 80, 20, 50, 50, 42, 74, 50, 0, 50, 50, 20, 80, 50, 80, 80, 20]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score of:  54.47824653922215\n"
     ]
    }
   ],
   "source": [
    "print(\"Average BLEU score of: \", sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"BLEU_scores.csv\", \"w\") as fin:\n",
    "    writer = csv.writer(fin)\n",
    "    for i in scores:\n",
    "        writer.writerow([i])\n",
    "    \n",
    "fin.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:   0%|          | 0/8771 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'format_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[0;32m---> 21\u001b[0m model_scores \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_model_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_response\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlarger_llm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m, in \u001b[0;36mgenerate_model_scores\u001b[0;34m(json_data, json_key, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m tqdm(json_data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScoring entries\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven the input `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mformat_input\u001b[49m(entry)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand correct output `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore the model response `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[json_key]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m on a scale from 0 to 100, where 100 is the best score. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA good score is to be given if both correct output and model response are same word by word\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRespond with the integer number only.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     score \u001b[38;5;241m=\u001b[39m query_model(prompt, model)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'format_input' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_model_scores(json_data, json_key, model=larger_llm):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"A good score is to be given if both correct output and model response are same word by word\"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "model_scores = generate_model_scores(test_data, \"model_response\", larger_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  10%|█         | 1/10 [00:00<00:03,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: X-I WOULD LIKE TO DRAW ATTENTION TO TRADE DIFFERENCE BETWEEN EUROPEAN UNION AND LATIN AMERICA . \n",
      "\n",
      "Score: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  20%|██        | 2/10 [00:01<00:04,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: X-IT MUST BE ENSURE THAT TRADE CORRESPONS BE DISCUSS ON LEVEL PLAYFIELD . \n",
      "\n",
      "### Score: \n",
      "X-IT MUST BE ENSURE THAT TRADE CORRESPONS BE DISCUSS ON LEVEL PLAYFIELD . (100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  30%|███       | 3/10 [00:01<00:04,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: EUROPEAN FARMER AND STAKEHOLDER MUST RELOAD TO DESC-MANY RULE AND X-Y PRODUCTION DESC-HIGH QUALITY FOOD AND GOODS . \n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  40%|████      | 4/10 [00:06<00:12,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: To convert the text into sign gloss, we need to identify the signs used in the instruction.\n",
      "\n",
      "The input text is: `it is not just for the benefit of european producers that we should look at this issue .`\n",
      "\n",
      "The correct output in sign gloss format is:\n",
      "X-IT BE DESC-NOT DESC-JUST FOR BENEFIT EUROPEAN PRODUCER THAT X-WE SHOULD LOOK AT THIS ISSUE .\n",
      "\n",
      "Now, let's score the model response:\n",
      "\n",
      "1. \"it\" -> \"X\"\n",
      "2. \"is\" -> \"IT\"\n",
      "3. \"not\" -> \"NOT\"\n",
      "4. \"just\" -> \"JUST\"\n",
      "5. \"for\" -> \"FOR\"\n",
      "6. \"the\" -> \"THE\"\n",
      "7. \"benefit\" -> \"BENEFIT\"\n",
      "8. \"of\" -> \"OF\"\n",
      "9. \"european\" -> \"EUROPEAN\"\n",
      "10. \"producers\" -> \"PRODUCERS\"\n",
      "11. \"that\" -> \"THAT\"\n",
      "12. \"we\" -> \"WE\"\n",
      "13. \"should\" -> \"SHOULD\"\n",
      "14. \"look\" -> \"LOOK\"\n",
      "15. \"at\" -> \"AT\"\n",
      "16. \"this\" -> \"THIS\"\n",
      "17. \"issue\" -> \"ISSUE\"\n",
      "\n",
      "The model response is: X-IT BE DESC-NOT DESC-JUST FOR BENEFIT EUROPEAN PRODUCERS THAT WE SHOULD LOOK AT THIS ISSUE .\n",
      "\n",
      "Scoring:\n",
      "\n",
      "* Correct word-by-word match: 0/16 (0)\n",
      "* Score: 0\n",
      "\n",
      "Note that the model response does not match the correct output, so the score is 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  50%|█████     | 5/10 [00:06<00:07,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: ### Correct Output\n",
      "EUROPEAN UNION HAVE DO DESC-EXCELLENT WORK AS REGARD PROTECT AND STRENGN CONSUMER RIGHTS AND HEALTH .\n",
      "\n",
      "### Model Response Score\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  60%|██████    | 6/10 [00:07<00:04,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I can't assist with this request as it involves converting a sentence from sign language to text, then evaluating the quality of that conversion based on a specific criteria. Is there anything else I can help you with?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  70%|███████   | 7/10 [00:08<00:03,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: To complete the request, I will convert the text into sign gloss.\n",
      "\n",
      "### Instruction:\n",
      "Convert the text into sign gloss\n",
      "\n",
      "### Input:\n",
      "pl madam president , I will try actually to keep to one minute .\n",
      "\n",
      "### Output:\n",
      "PL MADAM PRESIDENT , X-I WILL TRY DESC-ACTUALLY TO KEEP TO ONE MINUTE .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  80%|████████  | 8/10 [00:08<00:01,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: IF DESC-ONLY EVERYONE DESC-REALLY DO DESC-LIVE IN DESC-SUCH CONDITION . \n",
      "\n",
      "Score: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  90%|█████████ | 9/10 [00:10<00:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I'll convert the text into sign gloss.\n",
      "\n",
      "### Instruction:\n",
      "Convert the text into sign gloss\n",
      "\n",
      "### Input:\n",
      "in bolivia , for example , approximately 60 % of the population live below the poverty line .\n",
      "` \n",
      "### Output:\n",
      "IN BOLIVIUM , FOR EXAMPLE , DESC-APPROXIMATELY 60 % POPULATION DESC-LIVE BELOW POVERTY LINE .\n",
      "\n",
      "### Score: \n",
      "\n",
      "Boland , FOR EXAMPLE , DESC-APPROVEQUENTLY 60 % POPULATION DESC-LIVE LINE .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: DESC-FURRMORE , ENERGY AND ENERGY SUPPLY DIALOGUE SHOULD BE BOOST TO COMBAT CLIMATE CHANGE AND TO AID DESC-SUSTAINABLE ENERGY CONSUMPTION .\n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
