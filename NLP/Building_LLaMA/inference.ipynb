{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from model import ModelArgs, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class LLaMA:\n",
    "\n",
    "    def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_args = model_args\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
    "        prev_time = time.time()\n",
    "        if load_model:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*pth\"))\n",
    "            assert len(checkpoints) > 0, \"No checkpoints saved\"\n",
    "            chk_path = checkpoints[0]\n",
    "            print(f\"loading checkpoint :{chk_path}\")\n",
    "            checkpoint = torch.load(chk_path, map_location=\"cpu\")\n",
    "            print(f\"Loaded checkpoints in {time.time()-prev_time:.2f}s\")\n",
    "            prev_time = time.time()\n",
    "        with open(Path(checkpoints_dir) / \"param.json\", \"r\") as f:\n",
    "            params = json.load(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            device=device,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.vocab_size()\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "        \n",
    "        model = Transformer(model_args).to(device)\n",
    "\n",
    "        if load_model:\n",
    "            # The only unmatched key in the checkpoint is rope.freqs. Remove it\n",
    "            del checkpoint['rope.freqs']\n",
    "            model.load_state_dict(checkpoint, strict=True)\n",
    "            print(f\"Loaded state dict in {time.time() - prev_time:.2f}s\")\n",
    "        \n",
    "        return LLaMA(model, tokenizer, model_args)\n",
    "    \n",
    "    def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.args.max_seq_len - 1\n",
    "        # Convert each prompt into tokens\n",
    "        prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
    "        # Make sure the batch size is not too large\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert batch_size <= self.args.max_batch_size, f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        # Make sure the prompt length is not larger than the maximum sequence length\n",
    "        assert max_prompt_len <= self.args.max_seq_len, f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
    "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "        pad_id = self.tokenizer.pad_id()\n",
    "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            # Populate the initial tokens with the prompt tokens\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        \n",
    "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "        prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
    "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens\")\n",
    "        for cur_pos in cur_iterator:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
    "            if temperature > 0:\n",
    "                # The temperature is applied before the softmax\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = self._sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # Greedily select the token with the max probability\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # Only replace token if it is a padding token\n",
    "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            # EOS is reached only if we found an EOS token for a padding position\n",
    "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            # Cut to the EOS token, if present\n",
    "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
    "        return (out_tokens, out_text)\n",
    "    \n",
    "    def _sample_top_p(self, probs, p):\n",
    "        # (B, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # (B, vocab_size)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "        # (B, vocab_size)\n",
    "        # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "        mask = probs_sum - probs_sort > p \n",
    "        # Zero out all the probabilities of tokens that are not selected by the Top P\n",
    "        probs_sort[mask] = 0.0 \n",
    "        # Redistribute the probabilities so that they sum up to 1.\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # Sample a token (its index) from the top p distribution\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        # Get the token position in the vocabulary corresponding to the sampled index\n",
    "        next_token = torch.gather(probs_idx, -1, next_token) \n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint :/home/abhinav/Documents/Work/2 Hobby_projects/Models/LLaMA2/Llama2_7b/consolidated.00.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8680/261174300.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(chk_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    allow_cuda = False\n",
    "    device = 'cuda' if torch.cuda.is_available() and allow_cuda else 'cpu'\n",
    "\n",
    "    prompts = [\n",
    "        \"Simply put, the theory of relativity states that \",\n",
    "        \"If Google was an Italian company founded in Milan, it would\",\n",
    "        # Few shot promt\n",
    "        \"\"\"Translate English to French:\n",
    "        \n",
    "        sea otter => loutre de mer\n",
    "        peppermint => menthe poivrée\n",
    "        plush girafe => girafe peluche\n",
    "        cheese =>\"\"\",\n",
    "        # Zero shot prompt\n",
    "        \"\"\"Tell me if the following person is actually Doraemon disguised as human:\n",
    "        Name: Umar Jamil\n",
    "        Decision: \n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    model = LLaMA.build(\n",
    "        checkpoints_dir='/home/abhinav/Documents/Work/2 Hobby_projects/Models/LLaMA2/Llama2_7b',\n",
    "        tokenizer_path='/home/abhinav/Documents/Work/2 Hobby_projects/Models/LLaMA2/Llama2_7b/tokenizer.model',\n",
    "        load_model=True,\n",
    "        max_seq_len=1024,\n",
    "        max_batch_size=len(prompts),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=64))\n",
    "    assert len(out_texts) == len(prompts)\n",
    "    for i in range(len(out_texts)):\n",
    "        print(f'{out_texts[i]}')\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
