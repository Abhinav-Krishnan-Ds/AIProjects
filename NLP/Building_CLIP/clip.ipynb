{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import vit\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Application', 'DateTime', 'Duration', 'Formatter', 'List', 'Marker', 'Number', 'String', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'action_manager', 'actions', 'application', 'autocomplete', 'base_list_box', 'color', 'color_mappings', 'command_bar', 'config_parser', 'denotation', 'env', 'event', 'exception', 'formatter', 'formatter_base', 'help', 'key_cache', 'keybinding_parser', 'list_batcher', 'loader', 'markers', 'multi_widget', 'option_parser', 'parse_options', 'pid_manager', 'process', 'readline', 'registry', 'task', 'task_list', 'uda', 'util', 'version', 'xdg']\n"
     ]
    }
   ],
   "source": [
    "print(dir(vit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, image_paths):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "        self.transform = torchvision.transforms.ToTensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        caption, mask = self.inputs[idx].items()\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": caption[\"input_ids\"],\n",
    "            \"mask\": mask[\"attention_mask\"]\n",
    "        }\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, proj_dim):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertModel(config=DistilBertConfig())\n",
    "        self.projection = nn.Linear(embed_dim, proj_dim)\n",
    "        self.layer_norm = nn.LayerNorm(proj_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        x = x[:, 0, :] # B, T[cls], E\n",
    "        \n",
    "        x = self.projection(x)\n",
    "\n",
    "        return self.layer_norm(x)\n",
    "    \n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, base_model, embed_dim, proj_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = base_model\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.projection = nn.Linear(embed_dim, proj_dim)\n",
    "        self.layer_norm = nn.LayerNorm(proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(self.model(x))\n",
    "        return self.layer_norm(x)\n",
    "    \n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ViT = VisionTransformer( \n",
    "            num_layers=8,\n",
    "            img_size=224,\n",
    "            emb_size=768,\n",
    "            patch_size=16,\n",
    "            num_head=6,\n",
    "            num_class=False).to(self.device)\n",
    "        self.image_encoder = ImageEncoder(base_model=ViT, embed_dim=768, proj_dim=256)\n",
    "        self.text_encoder = TextEncoder(embed_dim=768, proj_dim=256)\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.ones([])*np.log(1/7)).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        I_t = self.image_encoder(x[\"image\"])\n",
    "        T_t = self.text_encoder(x[\"input_ids\"], x[\"mask\"])\n",
    "\n",
    "        logits = I_t@T_t.T * torch.exp(self.temperature)\n",
    "\n",
    "        labels = torch.arange(I_t.size(0)).to(self.device)\n",
    "\n",
    "        loss_I = F.cross_entropy(logits.T, labels)\n",
    "        loss_T = F.cross_entropy(logits, labels)\n",
    "\n",
    "        loss = (loss_I + loss_T)/2.0 \n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    texts = [\"This is a sample sentence.\", \"This is another example.\"]\n",
    "\n",
    "    # train_data = CustomDataset(texts, image_path)\n",
    "    # train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "    \n",
    "    # Example Usage\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    inputs= tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    test = {\n",
    "    \"image\" : torch.rand(2, 3, 224, 224).to(device),\n",
    "    \"input_ids\" : inputs[\"input_ids\"],\n",
    "    \"mask\" : inputs[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "    model = CLIPModel().to(device)\n",
    "    loss, logits = model(test)\n",
    "    print(\"Loss:\", loss, \"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
