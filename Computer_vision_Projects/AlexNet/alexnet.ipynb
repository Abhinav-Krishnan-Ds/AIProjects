{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 03:08:48.774580: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-16 03:08:48.868232: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-16 03:08:48.868338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-16 03:08:48.871528: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-16 03:08:48.892103: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 03:08:50.752947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import urllib\n",
    "import requests\n",
    "import PIL.Image\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading ship synset\n",
    "ship_page = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03095699\")\n",
    "ship_soup = BeautifulSoup(ship_page.content, \"html.parser\")\n",
    "\n",
    "#downloading bike synset:\n",
    "bike_page = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03792782\")\n",
    "bike_soup = BeautifulSoup(bike_page.content, \"html.parser\")\n",
    "\n",
    "ship_soup_str = str(ship_soup)\n",
    "ship_soup_split_url = ship_soup_str.split(\"\\r\\n\")\n",
    "\n",
    "bike_soup_str = str(bike_soup)\n",
    "bike_soup_split_url = bike_soup_str.split(\"\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./content')\n",
    "os.mkdir('./content/train')\n",
    "os.mkdir('./content/train/ships')\n",
    "os.mkdir('./content/train/bikes')\n",
    "os.mkdir('./content/validation')\n",
    "os.mkdir('./content/validation/ships')\n",
    "os.mkdir('./content/validation/bikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"le: \", len(bike_soup_split_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hisfhihihh:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:00, 7145.32it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#train images for ship\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m progress \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_training_images)):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mship_soup_split_url\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m             I \u001b[38;5;241m=\u001b[39m url_to_image(ship_soup_split_url[progress])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image_rows, image_cols = 32, 32\n",
    "\n",
    "input_shape = (image_rows, image_cols, 3)\n",
    "\n",
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    return image\n",
    "\n",
    "n_training_images = 100\n",
    "\n",
    "#train images for ship\n",
    "for progress in tqdm(range(n_training_images)):\n",
    "    if not ship_soup_split_url[progress] == None:\n",
    "        try:\n",
    "            I = url_to_image(ship_soup_split_url[progress])\n",
    "            if len(I.shape) == 3:\n",
    "                save_path = \"./content/train/ships/img\" + str(progress) + \".jpeg\"\n",
    "                cv2.imwrite(save_path, I)\n",
    "        except:\n",
    "            None\n",
    "\n",
    "#train images for bikes\n",
    "for progress in tqdm(range(n_training_images)):\n",
    "    if not bike_soup_split_url[progress] == None:\n",
    "        try:\n",
    "            I = url_to_image(bike_soup_split_url[progress])\n",
    "            if len(I.shape) == 3:\n",
    "                save_path = \"./content/train/bikes/img\" + str(progress) + \".jpeg\"\n",
    "                cv2.imwrite(save_path, I)\n",
    "        except:\n",
    "            None\n",
    "\n",
    "#validation images for ships\n",
    "for progress in tqdm(range(50)):\n",
    "    if not ship_soup_split_url[n_training_images+progress] == None:\n",
    "        try:\n",
    "            I = url_to_image(ship_soup_split_url[n_training_images+progress])\n",
    "            if len(I.shape) == 3:\n",
    "                save_path = \"./content/validation/ships/img\" + str(progress) + \".jpeg\"\n",
    "                cv2.imwrite(save_path, I)\n",
    "        except:\n",
    "            None\n",
    "\n",
    "\n",
    "#validation images for bikes\n",
    "for progress in tqdm(range(50)):\n",
    "    if not bike_soup_split_url[n_training_images+progress] == None:\n",
    "        try:\n",
    "            I = url_to_image(bike_soup_split_url[n_training_images+progress])\n",
    "            if len(I.shape) == 3:\n",
    "                save_path = \"./content/validation/bikes/img\" + str(progress) + \".jpeg\"\n",
    "                cv2.imwrite(save_path, I)\n",
    "        except:\n",
    "            None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "\n",
    "class AlexNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            96, \n",
    "            kernel_size=(11, 11), \n",
    "            strides=4, \n",
    "            padding=\"valid\", \n",
    "            activation=\"relu\", \n",
    "            input_shape=input_shape,\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "        \n",
    "        self.add(MaxPooling2D(\n",
    "            pool_size=(3, 3),\n",
    "            strides = (2, 2),\n",
    "            padding = \"valid\",\n",
    "            data_format = None\n",
    "        ))\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            256,\n",
    "            kernel_size=(5, 5),\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "\n",
    "        self.add(MaxPooling2D(\n",
    "            pool_size=(3, 3),\n",
    "            strides = (2, 2),\n",
    "            padding=\"valid\",\n",
    "            data_format=None\n",
    "        ))\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            384,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            384,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            256,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "\n",
    "        self.add(MaxPooling2D(\n",
    "            pool_size=(3, 3),\n",
    "            strides=(2, 2),\n",
    "            padding=\"valid\",\n",
    "            data_format=None\n",
    "        ))\n",
    "\n",
    "        self.add(Flatten())\n",
    "\n",
    "        self.add(Dense(4096, activation=\"relu\"))\n",
    "        self.add(Dense(4096, activation=\"relu\"))\n",
    "        self.add(Dense(1000, activation=\"relu\"))\n",
    "        self.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "        self.compile(\n",
    "            optimizer=Adam(0.001),\n",
    "            loss = 'categorical_crossentropy',\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 03:09:24.181623: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.277411: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.278099: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.285524: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.286337: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.286888: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.442063: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.442585: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.442977: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-16 03:09:24.443334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2285 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 3 classes.\n",
      "Found 264 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "\n",
    "model = AlexNet((227, 227, 3), num_classes)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_dir = \"/home/abhinav/Documents/Work/Hobby/Datasets/Imagenet/synsets\"\n",
    "valid_dir = \"/home/abhinav/Documents/Work/Hobby/Datasets/Imagenet/valid_synsets\"\n",
    "\n",
    "Image_height = 227\n",
    "Image_width = 227\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (Image_height, Image_width),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    seed=1,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(Image_height, Image_width),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    seed=7,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "train_num = train_generator.samples\n",
    "valid_num = valid_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./myalexnetmodel.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './logs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './logs'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"./logs\")\n",
    "os.mkdir(\"./logs/fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 03:09:32.527778: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 03:09:35.524208: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-07-16 03:09:35.717960: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-07-16 03:09:36.997212: I external/local_xla/xla/service/service.cc:168] XLA service 0x709658c09cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-16 03:09:36.997271: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-07-16 03:09:37.008188: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721079577.251017   19452 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 65s 496ms/step - loss: 3.4223 - accuracy: 0.5558 - val_loss: 0.9590 - val_accuracy: 0.4961\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 54s 485ms/step - loss: 0.6487 - accuracy: 0.7243 - val_loss: 0.5488 - val_accuracy: 0.7852\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 56s 495ms/step - loss: 0.4790 - accuracy: 0.8161 - val_loss: 0.3175 - val_accuracy: 0.8984\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 55s 492ms/step - loss: 0.4209 - accuracy: 0.8343 - val_loss: 0.5191 - val_accuracy: 0.8320\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 55s 494ms/step - loss: 0.3895 - accuracy: 0.8567 - val_loss: 0.4537 - val_accuracy: 0.8477\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 51s 458ms/step - loss: 0.3334 - accuracy: 0.8766 - val_loss: 0.3147 - val_accuracy: 0.9102\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 40s 357ms/step - loss: 0.3085 - accuracy: 0.8830 - val_loss: 0.4650 - val_accuracy: 0.8320\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 40s 355ms/step - loss: 0.2532 - accuracy: 0.9006 - val_loss: 0.2526 - val_accuracy: 0.9219\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 40s 356ms/step - loss: 0.2807 - accuracy: 0.8981 - val_loss: 0.3985 - val_accuracy: 0.8867\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 49s 438ms/step - loss: 0.2509 - accuracy: 0.9144 - val_loss: 0.4447 - val_accuracy: 0.8359\n",
      "Model: \"alex_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 27, 27, 96)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 13, 13, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              4097000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 3003      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62381347 (237.97 MB)\n",
      "Trainable params: 62381347 (237.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      6\u001b[0m     train_generator,\n\u001b[1;32m      7\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 17\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel_dir\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dir' is not defined"
     ]
    }
   ],
   "source": [
    "log_dir = \"./logs/fit\" + datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=train_num//batch_size,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_num//batch_size,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
