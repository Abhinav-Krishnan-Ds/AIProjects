{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 11:10:59.579820: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-30 11:10:59.617353: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 11:10:59.617385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 11:10:59.618288: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 11:10:59.623611: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 11:11:00.420127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_malloc_async\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 11:11:03.073349: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 11:11:03.110943: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 11:11:03.111153: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(os.getenv('TF_GPU_ALLOCATOR'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization of selective search\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to images and folder with bounding boxes of all airplane locations in an image for all images\n",
    "images_path = \"/home/abhinav/Documents/Work/2 Hobby_projects/Datasets/Aiplanes_dataset_for_rcnn/Images\"\n",
    "label_path = \"/home/abhinav/Documents/Work/2 Hobby_projects/Datasets/Aiplanes_dataset_for_rcnn/Airplanes_Annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 700 images with name starting with airplane, and 33 with numbers\n",
    "#to hold the image name and their bb of all airplane locations\n",
    "img_bb = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in os.listdir(label_path):                        # It gives each element of label_path directory to here (label)\n",
    "    if label.split(\"_\")[0] == \"airplane\":                   # We are only considering these images\n",
    "        try:\n",
    "            file = pd.read_csv(label_path+\"/\"+label)        # Reading each csv file containing bb of each image\n",
    "            # print(\"label: \", label)\n",
    "            # print(\"len of file: \", len(file))\n",
    "            labels_for_image = []                           # To store the bb in integer format from csv\n",
    "            for row in file.iterrows():\n",
    "                # for k in row:\n",
    "                #     print(row[k][0].split(\" \"))\n",
    "                bb_each = list(row[1])[0].split(\" \")\n",
    "                for j in range(len(bb_each)):\n",
    "                    bb_each[j] = eval(bb_each[j])\n",
    "\n",
    "                labels_for_image.append(bb_each)            # Labels_for_image is the list with all bb of current image\n",
    "            img_bb[label.split(\".\")[0]] = labels_for_image  # We choose file name without jpg as key and labels_for_image as their value\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error in :\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images for training:  700\n"
     ]
    }
   ],
   "source": [
    "# img_bb is a dictionary that now contains image name as key and their true bb of each airplane as a list of lists as the value of dictionary\n",
    "print(\"Total images for training: \", len(img_bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image name:  airplane_201\n",
      "Number of labels:  26\n",
      "Labels:  [[22, 78, 31, 90], [29, 69, 48, 81], [44, 65, 59, 76], [58, 57, 68, 70], [122, 16, 138, 26], [134, 9, 146, 21], [93, 35, 102, 46], [79, 41, 93, 56], [69, 48, 79, 59], [27, 110, 39, 121], [38, 102, 48, 111], [48, 97, 62, 105], [61, 89, 76, 97], [73, 82, 86, 91], [87, 75, 96, 84], [106, 69, 118, 79], [118, 53, 133, 65], [133, 47, 145, 58], [144, 39, 154, 48], [116, 238, 127, 253], [126, 232, 143, 248], [136, 225, 152, 241], [154, 216, 164, 226], [165, 209, 179, 219], [166, 220, 174, 230], [152, 229, 165, 237]]\n",
      "Image name:  airplane_407\n",
      "Number of labels:  3\n",
      "Labels:  [[30, 90, 78, 131], [143, 6, 190, 46], [200, 180, 253, 236]]\n",
      "Image name:  airplane_343\n",
      "Number of labels:  2\n",
      "Labels:  [[122, 153, 197, 247], [79, 5, 147, 98]]\n",
      "Image name:  airplane_520\n",
      "Number of labels:  4\n",
      "Labels:  [[63, 116, 92, 137], [107, 91, 132, 115], [140, 128, 165, 156], [176, 230, 202, 254]]\n"
     ]
    }
   ],
   "source": [
    "# To check how data is stored\n",
    "c = 0\n",
    "for i in img_bb:\n",
    "    if c == 4:\n",
    "        break\n",
    "    print(\"Image name: \", i)\n",
    "    print(\"Number of labels: \", len(img_bb[i]))\n",
    "    print(\"Labels: \", img_bb[i])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele1n = \"\"\n",
    "elebb1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the area of intersection\n",
    "def gt(bb1, bb2):\n",
    "    assert bb1['x1'] < bb1['x2']\n",
    "    assert bb1['y1'] < bb1['y2']\n",
    "    assert bb2['x1'] < bb2['x2']\n",
    "    assert bb2['y1'] < bb2['y2']\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1   Reading each image from folder\\n2   we consider only images starting with name airplane\\n3   Performing selective search\\n4   Taking each region proposal of ss, Each region propasal of ss is given as an array as [x, y, w, h]\\n        where (x, y) are top left corner coordinates of bb and w, h are width and height\\n5   Here we do not add width and height as the label data cause:\\n6   we downloaded has bb of the format [x1, y1, x2, y2]\\n        (x1, y1): top left corner coordinates\\n        (x2, y2): bottom right corner coordinates\\n7   Here iou is greater than 0.7, if we haven\\'t added 30 of this class to train_images:\\n        add the resized image of this proposed bounding box by selective search to train_images\\n\\n\\n\\nSteps involved in displaying an image in opencv:\\n    cv2.imshow(\"imout: \", imout)\\n    cv2.waitKey(0),\\n    cv2.destroyAllWindows()\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1   Reading each image from folder\n",
    "2   we consider only images starting with name airplane\n",
    "3   Performing selective search\n",
    "4   Taking each region proposal of ss, Each region propasal of ss is given as an array as [x, y, w, h]\n",
    "        where (x, y) are top left corner coordinates of bb and w, h are width and height\n",
    "5   Here we do not add width and height as the label data cause:\n",
    "6   we downloaded has bb of the format [x1, y1, x2, y2]\n",
    "        (x1, y1): top left corner coordinates\n",
    "        (x2, y2): bottom right corner coordinates\n",
    "7   Here iou is greater than 0.7, if we haven't added 30 of this class to train_images:\n",
    "        add the resized image of this proposed bounding box by selective search to train_images\n",
    "\n",
    "\n",
    "\n",
    "Steps involved in displaying an image in opencv:\n",
    "    cv2.imshow(\"imout: \", imout)\n",
    "    cv2.waitKey(0),\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we pass each image for selective search and calculate iou for each bb output of ss out of 2000 proposals\n",
    "\n",
    "x_new_images, y_new_labels = [], []\n",
    "no_images = 0\n",
    "no_images_to_be_used_for_extraction = 30\n",
    "total_images_extracted = 0\n",
    "for img in os.listdir(images_path):             # 1\n",
    "    if no_images == no_images_to_be_used_for_extraction:   # We are only 10 images, 30 resized images each for foreground and background are taken from each of these 10 images\n",
    "                                                # Total of 300 positive images and 300 negative samples are given for VGGNet to train\n",
    "        break\n",
    "    try:\n",
    "        if img.split(\"_\")[0] == \"airplane\":                              # 2\n",
    "\n",
    "            image = cv2.imread(os.path.join(images_path, img))\n",
    "\n",
    "            ss.setBaseImage(image)                                       # 3\n",
    "            ss.switchToSelectiveSearchFast(base_k=10)\n",
    "            ssresults = ss.process()\n",
    "            imout = image.copy()\n",
    "\n",
    "            # cv2.imshow(\"imout: \", imout)\n",
    "            # cv2.waitKey(0),\n",
    "            # cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "            # ele1n = os.path.join(images_path, img)\n",
    "\n",
    "            # In openCV, consider a square window facing us, \n",
    "            # its left side will be y axis and top side will be x axis\n",
    "            # top left corner is considered as (0, 0)\n",
    "\n",
    "            iou_07 = 0\n",
    "            iou_03 = 0\n",
    "            for i in ssresults:\n",
    "                x1, y1, w, h = i                                          # 4\n",
    "                bb2 = {\"x1\":x1, \"y1\":y1, \"x2\":x1+w, \"y2\":y1+h}            \n",
    "                img = img.split(\".\")[0]\n",
    "                for j in img_bb[img]:                                     # For each true bb in img_bb, we calculate iou for the bb region proposed by ss\n",
    "                    x, y, xn, yn = j                                      # 5\n",
    "                    bb1 = {\"x1\":x, \"y1\":y, \"x2\":xn, \"y2\":yn}              # 6\n",
    "                    iou = gt(bb1, bb2)                                    # Calculating iou\n",
    "                    if iou > 0.7 and iou_07 < 30:                         # 7\n",
    "                        # print(\"iou07: bb: x1, y1, w, h, x1+w, y1+h:\", x1, y1, w, h, x1+w, y1+h)\n",
    "                        timage = imout[y1:y1+h,x1:x1+w]\n",
    "                        resized = cv2.resize(timage, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "                        x_new_images.append(resized)\n",
    "                        y_new_labels.append(1)\n",
    "                        iou_07 += 1\n",
    "                        # cv2.imshow(\"resized: \", resized)\n",
    "                        # cv2.waitKey(0)\n",
    "                        # cv2.destroyAllWindows()\n",
    "                        # cv2.rectangle(image, (bb1[\"x1\"], bb1[\"y1\"]), (bb1[\"x2\"], bb1[\"y2\"]), (255, 0, 0), 2)\n",
    "                        # cv2.rectangle(image, (bb2[\"x1\"], bb2[\"y1\"]), (bb2[\"x2\"], bb2[\"y2\"]), (0, 255, 0), 1)\n",
    "                    elif iou < 0.3 and iou_03 < 30:\n",
    "                        timage = imout[y1:y1+h, x1:x1+h]\n",
    "                        resized = cv2.resize(timage, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "                        x_new_images.append(resized)\n",
    "                        y_new_labels.append(0)\n",
    "                        iou_03 += 1\n",
    "            # cv2.imshow(img, image)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "            # print(\"for image: \", img, \" there are _ iou07 and _ iou03: \", iou_07, iou_03)\n",
    "            total_images_extracted += iou_03 + iou_07\n",
    "        no_images += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error in image: \", img)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines can display the image you want\n",
    "\n",
    "# i = cv2.imread(\"/home/abhinav/Documents/Work/2 Hobby_projects/Datasets/Aiplanes_dataset_for_rcnn/Images/airplane_343.jpg\")\n",
    "# i = cv2.imread(\"/home/abhinav/Documents/Work/2 Hobby_projects/Datasets/Aiplanes_dataset_for_rcnn/Images/42845.jpg\")\n",
    "\n",
    "# cv2.imshow(\"image\", i)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images extracted and resized:  1246\n",
      "len of x_new_images:  1246\n",
      "len of y_new_labels:  1246\n"
     ]
    }
   ],
   "source": [
    "# Now we have in total 1246 images of positive and negative samples mix to fine tune the next VGGNet model\n",
    "\n",
    "print(\"total images extracted and resized: \", total_images_extracted)\n",
    "print(\"len of x_new_images: \", len(x_new_images))\n",
    "print(\"len of y_new_labels: \", len(y_new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying images of x_new_images resized images:\n",
    "\n",
    "# for index, i in enumerate(x_new_images):\n",
    "#     cv2.imshow(\"image: \" + str(y_new_labels[index]), i)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting resized images to array by numpy to fine tune model\n",
    "\n",
    "x_new = np.array(x_new_images)\n",
    "y_new = np.array(y_new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = np.array(y_new)\n",
    "\n",
    "# lb = LabelBinarizer()\n",
    "# lb.fit(range(max(arr)+1))\n",
    "# encoded_y = lb.transform(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in encoded_y:\n",
    "#     print(i, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]], shape=(1246, 2), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 11:11:12.388118: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 11:11:12.388322: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 11:11:12.388395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 11:11:12.437975: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 11:11:12.438228: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 11:11:12.438293: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-07-30 11:11:12.438491: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 11:11:12.438641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2285 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "arr = np.array(y_new)\n",
    "C = tf.constant(2)\n",
    "\n",
    "encoded_y = tf.one_hot(arr, C, axis=-1)\n",
    "\n",
    "print(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xnew:  1246\n",
      "xnew shape:  (1246, 224, 224, 3)\n",
      "enc_y 1246\n"
     ]
    }
   ],
   "source": [
    "print(\"xnew: \", len(x_new))\n",
    "print(\"xnew shape: \", x_new.shape)\n",
    "print(\"enc_y\", len(encoded_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "\n",
    "class AlexNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            96, \n",
    "            kernel_size=(11, 11), \n",
    "            strides=4, \n",
    "            padding=\"valid\", \n",
    "            activation=\"relu\", \n",
    "            input_shape=input_shape,\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "        \n",
    "        self.add(MaxPooling2D(\n",
    "            pool_size=(3, 3),\n",
    "            strides = (2, 2),\n",
    "            padding = \"valid\",\n",
    "            data_format = None\n",
    "        ))\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            256,\n",
    "            kernel_size=(5, 5),\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "\n",
    "        self.add(MaxPooling2D(\n",
    "            pool_size=(3, 3),\n",
    "            strides = (2, 2),\n",
    "            padding=\"valid\",\n",
    "            data_format=None\n",
    "        ))\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            384,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            384,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "\n",
    "        self.add(Conv2D(\n",
    "            256,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "\n",
    "        self.add(MaxPooling2D(\n",
    "            pool_size=(3, 3),\n",
    "            strides=(2, 2),\n",
    "            padding=\"valid\",\n",
    "            data_format=None\n",
    "        ))\n",
    "\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Flatten())\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(4096, activation=\"relu\"))\n",
    "        self.add(Dense(4096, activation=\"relu\"))\n",
    "        self.add(Dense(1000, activation=\"relu\"))\n",
    "        self.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "        self.compile(\n",
    "            optimizer=Adam(0.0001),\n",
    "            loss = 'categorical_crossentropy',\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alex_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 26, 26, 96)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 384)       885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 384)       1327488   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 256)       884992    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 5, 5, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5, 5, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              26218496  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              4097000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 2002      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50846010 (193.96 MB)\n",
      "Trainable params: 50846010 (193.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet((224, 224, 3), 2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/abhinav/Documents/Work/Hobby/Models/myalexnet.tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1246, 224, 224, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = x_new[:1000], x_new[1000:1246], encoded_y[:1000], encoded_y[1000:1246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 100\n",
    "train_num = 1246\n",
    "valid_num = 1246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 11:11:13.091392: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-07-30 11:11:13.943050: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inalex_net/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-07-30 11:11:14.134734: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-07-30 11:11:14.209523: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-07-30 11:11:16.444958: I external/local_xla/xla/service/service.cc:168] XLA service 0x5b3ca54ecc10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-30 11:11:16.444977: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-07-30 11:11:16.448597: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722318076.516785   17475 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 31s 1s/step - loss: 350.6305 - accuracy: 0.5326 - val_loss: 1.8747 - val_accuracy: 0.7200\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6.4978 - accuracy: 0.6682 - val_loss: 0.3684 - val_accuracy: 0.9200\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 1.4995 - accuracy: 0.7975 - val_loss: 0.2728 - val_accuracy: 0.9120\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 0.6853 - accuracy: 0.8492 - val_loss: 0.2651 - val_accuracy: 0.9200\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 0.4924 - accuracy: 0.8822 - val_loss: 0.3098 - val_accuracy: 0.8880\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 0.4639 - accuracy: 0.8769 - val_loss: 0.6495 - val_accuracy: 0.8080\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.3733 - accuracy: 0.8974 - val_loss: 0.5288 - val_accuracy: 0.8000\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.3253 - accuracy: 0.8876 - val_loss: 0.2478 - val_accuracy: 0.9200\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 0.2776 - accuracy: 0.9028 - val_loss: 0.3734 - val_accuracy: 0.8880\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 0.2288 - accuracy: 0.9224 - val_loss: 0.3520 - val_accuracy: 0.9040\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 0.1967 - accuracy: 0.9224 - val_loss: 0.2807 - val_accuracy: 0.9280\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 0.2218 - accuracy: 0.9286 - val_loss: 0.2910 - val_accuracy: 0.9360\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 0.1856 - accuracy: 0.9402 - val_loss: 0.3234 - val_accuracy: 0.9440\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 0.2017 - accuracy: 0.9340 - val_loss: 0.3721 - val_accuracy: 0.9120\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.2115 - accuracy: 0.9295 - val_loss: 0.3573 - val_accuracy: 0.9280\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 0.1697 - accuracy: 0.9429 - val_loss: 0.3104 - val_accuracy: 0.9200\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.1448 - accuracy: 0.9384 - val_loss: 0.3141 - val_accuracy: 0.9200\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.1452 - accuracy: 0.9438 - val_loss: 0.2119 - val_accuracy: 0.9360\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 0.1320 - accuracy: 0.9483 - val_loss: 0.2697 - val_accuracy: 0.9440\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 0.1081 - accuracy: 0.9554 - val_loss: 0.3504 - val_accuracy: 0.9120\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.1355 - accuracy: 0.9616 - val_loss: 0.3290 - val_accuracy: 0.9200\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 0.0976 - accuracy: 0.9661 - val_loss: 0.3138 - val_accuracy: 0.9360\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.0893 - accuracy: 0.9652 - val_loss: 0.2997 - val_accuracy: 0.9440\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 0.0875 - accuracy: 0.9661 - val_loss: 0.2569 - val_accuracy: 0.9360\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.1072 - accuracy: 0.9634 - val_loss: 0.3313 - val_accuracy: 0.9200\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 0.0780 - accuracy: 0.9688 - val_loss: 0.3124 - val_accuracy: 0.9200\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.0858 - accuracy: 0.9723 - val_loss: 0.2044 - val_accuracy: 0.9280\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 0.0961 - accuracy: 0.9679 - val_loss: 0.1931 - val_accuracy: 0.9360\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 0.0791 - accuracy: 0.9697 - val_loss: 0.1823 - val_accuracy: 0.9520\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 0.0911 - accuracy: 0.9741 - val_loss: 0.2562 - val_accuracy: 0.9440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7e70100915a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = \"./logs/fit\" + datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    x=x_new,\n",
    "    y = encoded_y,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=train_num//batch_size,\n",
    "    validation_split=0.1,\n",
    "    validation_steps=valid_num//batch_size,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggmodel = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layers in (vggmodel.layers)[:15]:\n",
    "#     print(layers)\n",
    "#     layers.trainable = False\n",
    "#     X= vggmodel.layers[-2].output\n",
    "# predictions = Dense(2, activation=\"softmax\")(X)\n",
    "# model_final = Model(vggmodel.input, predictions)\n",
    "# opt = Adam(lr=0.0001)\n",
    "# model_final.compile(loss = keras.losses.categorical_crossentropy, optimizer = opt, metrics=[\"accuracy\"])\n",
    "# model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyLabelBinarizer(LabelBinarizer):\n",
    "#     def transform(self, y):\n",
    "#         Y = super().transform(y)\n",
    "#         if self.y_type_ == 'binary':\n",
    "#             return np.hstack((Y, 1-Y))\n",
    "#         else:\n",
    "#             return Y\n",
    "#     def inverse_transform(self, Y, threshold=None):\n",
    "#         if self.y_type_ == 'binary':\n",
    "#             return super().inverse_transform(Y[:, 0], threshold)\n",
    "#         else:\n",
    "#             return super().inverse_transform(Y, threshold)\n",
    "# lenc = MyLabelBinarizer()\n",
    "# Y =  lenc.fit_transform(y_new)\n",
    "# X_train, X_test , y_train, y_test = train_test_split(x_new,Y,test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trdata = ImageDataGenerator(\n",
    "#     horizontal_flip=True, \n",
    "#     vertical_flip=True, \n",
    "#     rotation_range=90\n",
    "#     )\n",
    "# traindata = trdata.flow(\n",
    "#     x=X_train, \n",
    "#     y=y_train\n",
    "#     )\n",
    "# tsdata = ImageDataGenerator(\n",
    "#     horizontal_flip=True, \n",
    "#     vertical_flip=True, \n",
    "#     rotation_range=90\n",
    "#     )\n",
    "# testdata = tsdata.flow(\n",
    "#     x=X_test, \n",
    "#     y=y_test\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     \"ieeercnn_vgg16_1.h5\", \n",
    "#     monitor='val_loss', \n",
    "#     verbose=1, \n",
    "#     save_best_only=True, \n",
    "#     save_weights_only=False, \n",
    "#     mode='auto', \n",
    "#     period=1)\n",
    "# early = EarlyStopping(\n",
    "#     monitor='val_loss', \n",
    "#     min_delta=0, \n",
    "#     patience=100, \n",
    "#     verbose=1, \n",
    "#     mode='auto'\n",
    "#     )\n",
    "# hist = model_final.fit(\n",
    "#     traindata, \n",
    "#     steps_per_epoch= 10, \n",
    "#     epochs= 100, \n",
    "#     validation_data= testdata, \n",
    "#     validation_steps=10, \n",
    "#     callbacks=[checkpoint,early]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images that starts with 4 :  17\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for img in os.listdir(images_path):\n",
    "    if img.startswith(\"4\"):\n",
    "        c+= 1\n",
    "print(\"Number of images that starts with 4 : \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  238 187\n",
      "shape :  (37, 82, 3)\n",
      "r shape:  (224, 224, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"alex_net\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, 224, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m im_to_m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(im_to_model, (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m) , interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_AREA)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, im_to_m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_to_m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mres: \u001b[39m\u001b[38;5;124m\"\u001b[39m, res)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# cv2.imshow(\"image\", im_to_model)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# cv2.waitKey(0)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# cv2.destroyAllWindows()\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filea4jfzs8c.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/abhinav/.local/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"alex_net\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "\n",
    "for img in os.listdir(images_path):\n",
    "    if img.startswith(\"4\"):\n",
    "        image = cv2.imread(os.path.join(images_path, img))\n",
    "        # cv2.imshow(\"image\", image)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "        ss.setBaseImage(image)\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "        ssresults = ss.process()\n",
    "        imout = image.copy()\n",
    "        for bb in ssresults:\n",
    "            if cnt > 1:\n",
    "                break\n",
    "            x, y, w, h = bb\n",
    "            im_to_model = imout[y:y+h, x:x+w]\n",
    "            print(\"shape: \", x+w, y+h)\n",
    "            print(\"shape : \", im_to_model.shape)\n",
    "            im_to_m = cv2.resize(im_to_model, (224, 224) , interpolation=cv2.INTER_AREA)\n",
    "            print(\"r shape: \", im_to_m.shape)\n",
    "            res = model.predict(im_to_m)\n",
    "            print(\"res: \", res)\n",
    "            # cv2.imshow(\"image\", im_to_model)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1   Reading each image from folder\n",
    "2   we consider only images starting with name airplane\n",
    "3   Performing selective search\n",
    "4   Taking each region proposal of ss, Each region propasal of ss is given as an array as [x, y, w, h]\n",
    "        where (x, y) are top left corner coordinates of bb and w, h are width and height\n",
    "5   Here we do not add width and height as the label data cause:\n",
    "6   we downloaded has bb of the format [x1, y1, x2, y2]\n",
    "        (x1, y1): top left corner coordinates\n",
    "        (x2, y2): bottom right corner coordinates\n",
    "7   Here iou is greater than 0.7, if we haven't added 30 of this class to train_images:\n",
    "        add the resized image of this proposed bounding box by selective search to train_images\n",
    "\n",
    "\n",
    "\n",
    "Steps involved in displaying an image in opencv:\n",
    "    cv2.imshow(\"imout: \", imout)\n",
    "    cv2.waitKey(0),\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m ss\u001b[38;5;241m.\u001b[39msetBaseImage(image)                                       \u001b[38;5;66;03m# 3\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ss\u001b[38;5;241m.\u001b[39mswitchToSelectiveSearchFast(base_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m ssresults \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m imout \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# cv2.imshow(\"imout: \", imout)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# cv2.waitKey(0),\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# cv2.destroyAllWindows()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# its left side will be y axis and top side will be x axis\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# top left corner is considered as (0, 0)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #we pass each image for selective search and calculate iou for each bb output of ss out of 2000 proposals\n",
    "\n",
    "# x_new_images, y_new_labels = [], []\n",
    "# no_images = 0\n",
    "# no_images_to_be_used_for_extraction = 30\n",
    "# total_images_extracted = 0\n",
    "# for img in os.listdir(images_path):             # 1\n",
    "#     if no_images == no_images_to_be_used_for_extraction:   # We are only 10 images, 30 resized images each for foreground and background are taken from each of these 10 images\n",
    "#                                                 # Total of 300 positive images and 300 negative samples are given for VGGNet to train\n",
    "#         break\n",
    "#     try:\n",
    "#         if img.split(\"_\")[0] == \"airplane\":                              # 2\n",
    "\n",
    "#             image = cv2.imread(os.path.join(images_path, img))\n",
    "\n",
    "#             ss.setBaseImage(image)                                       # 3\n",
    "#             ss.switchToSelectiveSearchFast(base_k=10)\n",
    "#             ssresults = ss.process()\n",
    "#             imout = image.copy()\n",
    "\n",
    "#             # cv2.imshow(\"imout: \", imout)\n",
    "#             # cv2.waitKey(0),\n",
    "#             # cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#             # ele1n = os.path.join(images_path, img)\n",
    "\n",
    "#             # In openCV, consider a square window facing us, \n",
    "#             # its left side will be y axis and top side will be x axis\n",
    "#             # top left corner is considered as (0, 0)\n",
    "\n",
    "#             iou_07 = 0\n",
    "#             iou_03 = 0\n",
    "#             for i in ssresults:\n",
    "#                 x1, y1, w, h = i                                          # 4\n",
    "#                 bb2 = {\"x1\":x1, \"y1\":y1, \"x2\":x1+w, \"y2\":y1+h}            \n",
    "#                 img = img.split(\".\")[0]\n",
    "#                 for j in img_bb[img]:                                     # For each true bb in img_bb, we calculate iou for the bb region proposed by ss\n",
    "#                     x, y, xn, yn = j                                      # 5\n",
    "#                     bb1 = {\"x1\":x, \"y1\":y, \"x2\":xn, \"y2\":yn}              # 6\n",
    "#                     iou = gt(bb1, bb2)                                    # Calculating iou\n",
    "#                     if iou > 0.7 and iou_07 < 30:                         # 7\n",
    "#                         # print(\"iou07: bb: x1, y1, w, h, x1+w, y1+h:\", x1, y1, w, h, x1+w, y1+h)\n",
    "#                         timage = imout[y1:y1+h,x1:x1+w]\n",
    "#                         resized = cv2.resize(timage, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "#                         x_new_images.append(resized)\n",
    "#                         y_new_labels.append(1)\n",
    "#                         iou_07 += 1\n",
    "#                         # cv2.imshow(\"resized: \", resized)\n",
    "#                         # cv2.waitKey(0)\n",
    "#                         # cv2.destroyAllWindows()\n",
    "#                         # cv2.rectangle(image, (bb1[\"x1\"], bb1[\"y1\"]), (bb1[\"x2\"], bb1[\"y2\"]), (255, 0, 0), 2)\n",
    "#                         # cv2.rectangle(image, (bb2[\"x1\"], bb2[\"y1\"]), (bb2[\"x2\"], bb2[\"y2\"]), (0, 255, 0), 1)\n",
    "#                     elif iou < 0.3 and iou_03 < 30:\n",
    "#                         timage = imout[y1:y1+h, x1:x1+h]\n",
    "#                         resized = cv2.resize(timage, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "#                         x_new_images.append(resized)\n",
    "#                         y_new_labels.append(0)\n",
    "#                         iou_03 += 1\n",
    "#             # cv2.imshow(img, image)\n",
    "#             # cv2.waitKey(0)\n",
    "#             # cv2.destroyAllWindows()\n",
    "#             # print(\"for image: \", img, \" there are _ iou07 and _ iou03: \", iou_07, iou_03)\n",
    "#             total_images_extracted += iou_03 + iou_07\n",
    "#         no_images += 1\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(\"Error in image: \", img)\n",
    "#         continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
