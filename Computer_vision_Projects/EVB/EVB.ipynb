{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 10:44:30.040851: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-15 10:44:30.147928: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 10:44:30.147981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 10:44:30.166497: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 10:44:30.214607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 10:44:30.986293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "train_images = train_images.reshape((60000, 784))\n",
    "train_images = train_images.astype('float32')/255\n",
    "test_images = test_images.reshape((10000, 784))\n",
    "test_images = test_images.astype('float32')/255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " ebv__layer_8 (EBV_Layer)    (None, 10)                640       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 575040 (2.19 MB)\n",
      "Trainable params: 575040 (2.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1688/1688 [==============================] - 8s 4ms/step - loss: 7.8875 - acc: 0.1663 - val_loss: 7.7041 - val_acc: 0.1300\n",
      "Epoch 2/6\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 8.2565 - acc: 0.1303 - val_loss: 6.2746 - val_acc: 0.1573\n",
      "Epoch 3/6\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 7.4272 - acc: 0.1146 - val_loss: 6.4768 - val_acc: 0.1027\n",
      "Epoch 4/6\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 6.3804 - acc: 0.1045 - val_loss: 6.3747 - val_acc: 0.0988\n",
      "Epoch 5/6\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 6.2690 - acc: 0.1035 - val_loss: 6.9711 - val_acc: 0.0992\n",
      "Epoch 6/6\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 6.7114 - acc: 0.1040 - val_loss: 6.1437 - val_acc: 0.1000\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.1877 - acc: 0.1051\n",
      "Accuracy:  0.10509999841451645  loss:  6.187736511230469\n"
     ]
    }
   ],
   "source": [
    "class EBV_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(EBV_Layer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.kernal = self.add_weight(name='kernal',\n",
    "                                      shape = (input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(EBV_Layer, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        kernal_norm = tf.norm(self.kernal, axis=0, keepdims=True)\n",
    "        kernal_norm = tf.maximum(kernal_norm, tf.keras.backend.epsilon())\n",
    "        kernal_unit_norm = self.kernal/kernal_norm\n",
    "        output = tf.matmul(x, kernal_unit_norm)\n",
    "        return output\n",
    "    \n",
    "model1 = models.Sequential()\n",
    "# model1.add(Flatten())\n",
    "model1.add(Dense(512, input_dim=784, activation='relu'))\n",
    "model1.add(Dense(256, activation=\"relu\"))\n",
    "model1.add(Dense(128, activation=\"relu\"))\n",
    "model1.add(Dense(64, activation=\"relu\"))\n",
    "model1.add(EBV_Layer(10, input_shape=(64,)))\n",
    "\n",
    "model1.compile(optimizer=Nadam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "\n",
    "history = model1.fit(train_images,\n",
    "                     train_labels,\n",
    "                     batch_size=32,\n",
    "                     epochs=6,\n",
    "                     validation_split=0.1\n",
    "                     )\n",
    "\n",
    "result = model1.evaluate(test_images, test_labels)\n",
    "print(\"Accuracy: \", result[1], \" loss: \", result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
